# Architecture of RNN

The below image show a RNN being unfolded to a full network, x\_t is input to the network at time t, h\_t is the hidden state at time t also referred to as the memory of the network. It is calculated based on previous hidden state and current input.

Represented by

â„\_ğ‘¡=ğ‘“\(ğ‘ˆğ‘¥\_ğ‘¡+ğ‘Šâ„\_ğ‘¡âˆ’1+ğ‘\_â„\)  


![](../.gitbook/assets/rnn.png)

Here U and W are weights for input and previous state value input, ğ‘â„bh is the bias associated to the hidden network and f is the non-linearity applied to the sum to generate final cell state.

And ou**t**put at time t is calculated as shown below :

ğ‘‚\_ğ‘¡=ğ‘“\(ğ‘Šâ„\_ğ‘¡+ğ‘\_0\)

b\_0 is the bias for the output layer

